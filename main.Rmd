---
title: "Stilized Facts about NBER Working Papers: a fully reproducible analysis"
author: "Fernando Hoces de la Guardia"
date: "1/29/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv("DATAVERSE_KEY" = "examplekey12345")
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")


#tidyverse, dataverse
```

## Web Scrapping (optional)

First: we obtain the data from the NBER website. This task is achieve through the following steps:
 1 - Identify the last paper pubished  
 2 - Run a loop that iterates over the webpage of each paper. For each paper: 
  2.1 - Get the authors and store it the variable `authors`. When multiple authors, separate them with the "-*-" symbol  
  2.2 - Get the title, date of publication, abstract and identify if the working paper was ever published. Use this information to create the variables `title`, `date`, `abstract`, `published`. 
  2.3 - Get the NBER categories of study and store it the variable `NBER_cat`. When multiple authors, separate them with the "-*-" symbol.  
 3 - Save the scraped data set and upload it to repository on dataverse [IN PROGRESS]. 

 


```{r web  scrapping1}
# Define function to the last working paper
last.wp.f <- function() {
  # Download the NBER news web page
  readLines("https://www.nber.org/new.html") %>% 
    # then pull al the strings of the form "w[0-9]{5}"
    str_extract("w[0-9]{5}") %>%
    # then keep only the numeric part of those strings. 
    str_extract("[0-9]{5}") %>% 
    # transform to numeric
    as.numeric() %>% 
    # find the maximum
    max(na.rm = TRUE)
}

# Get the last working paper
last.wp <- last.wp.f()
last.wp <- 10
# Define a vector that enumerates all possible papers
papers <- 1:last.wp

## Define function toget the extracted lines that match "pattern", and 
## keep only the second group according to "group_pat"
get.line.f <- function(pattern, group_pat, data_str = raw_lines) {
  matched.lines <- grep(pattern, data_str, value = TRUE)
  sub(group_pat,"\\2", matched.lines) 
}

scrape.nber.f <- function(papers.var){
  # Build a empty data set that will contain all the information of each paper (row) 
  df <- data.frame(authors = rep(NA, length(papers.var)), 
           title  = rep(NA, length(papers.var)), 
           date = rep(NA, length(papers.var)), 
           abstract = rep(NA, length(papers.var)), 
           published = rep(NA, length(papers.var)),
           NBER_cat = rep(NA, length(papers.var)))
  # "j" tracks the data that stores the results  
  j <- 0
  for (i in papers.var) {
    j <- j + 1
    ## read in the website that contains links to all the pages where we want to download data
    raw_lines <- tryCatch(
                readLines( paste("https://www.nber.org/papers/w", i, sep = "") ), 
                error = function(e) NULL)
    # If there is no such paper, jump to next one
    while (is.null(raw_lines)) {
      i <- i + 1
      j <- j + 1
      raw_lines <- tryCatch(
        readLines(paste("https://www.nber.org/papers/w", i, sep = "")), 
        error = function(e) NULL)
      }
  
    df$authors[j] <- get.line.f('<meta name="citation_author" content=.*', 
               '(<meta name=\"citation_author\" content=\")(.*)(\">)') %>% 
        paste(collapse = "-*-" )
  
    df$title[j] <- get.line.f('<meta name="citation_title" content=.*' , 
               '(<meta name="citation_title" content=\")(.*)(\">)')
        
    df$date[j] <- get.line.f('<meta name="DC.Date" content=.*', 
               '(<meta name="DC.Date" content=\")(.*)(\">)')
    
    df$abstract[j]<- raw_lines[which.max(nchar(raw_lines))]
    
    df$published[j] <- ifelse(length(grep("<p id='published_line'>.*"
                                          , raw_lines, value = TRUE)) == 0,
                              "not published", "published")
    # Get lines with NBER categories and clean them
    clean.categ <- grep("<b>NBER Program.*", raw_lines, value = TRUE) %>% 
      str_extract_all("([A-Z]{1,4})\\.html") %>% 
      str_extract_all("[A-Z]{1,4}") %>% 
      unlist() %>% paste(collapse = "-*-")
    
    df$NBER_cat[j] <- ifelse(length(clean.categ) == 0,
                              "No Category", clean.categ)
    # Track progress
    if (j%%50 == 0) {
      print(paste(round(j/length(papers.var), 3) * 100,"% done", sep = ""))
    } 
  }
  return(df)
}

# run the scrapping function
start.time <- Sys.time()
df <- scrape.nber.f(papers.var = papers + 2e4)
print(Sys.time() - start.time)


# Identify the data: (apply some hash function)

# upload to dataverse
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")
Sys.setenv("DATAVERSE_KEY" = "b4d2c057-79a6-42ad-8749-9f9c2b711499")

# create a dataverse
dat <- create_dataverse("mydataverse")


# create a list of metadata
metadat <- list(title = "My Study",
                creator = "Doe, John",
                description = "An example study")

# create a list of metadata
metadat <- list(title = "My Study",
                creator = "Doe, John",
                description = "An example study")

# create the dataset
dat <- initiate_dataset("dataverse.harvard.edu", body = metadat)

# 
# create the dataset
ds <- create_dataset("mydataverse")

# add files
tmp <- tempfile()
write.csv(iris, file = tmp)
f <- add_dataset_file(file = tmp, dataset = ds)

# publish dataset
publish_dataset(ds)

# dataset will now be published
get_dataverse("mydataverse")

```



